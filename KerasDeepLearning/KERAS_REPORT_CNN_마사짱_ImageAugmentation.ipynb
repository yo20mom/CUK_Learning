{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KERAS_REPORT_CNN_마사짱_ImageAugmentation.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNU9tOcJA5s1447O3MXAUf4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yo20mom/CUK_Learning/blob/master/KerasDeepLearning/KERAS_REPORT_CNN_%EB%A7%88%EC%82%AC%EC%A7%B1_ImageAugmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bRErQIN8_Oy"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, utils, callbacks, datasets, layers, applications, optimizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.datasets import mnist, fashion_mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, AvgPool2D, BatchNormalization, Activation, MaxPooling2D\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras import backend as K\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "No4hxMSpBZSe"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsekGcJQkxro"
      },
      "source": [
        "# 이미지 생성을 위한 generator 추가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-u8ZqGhkKpL"
      },
      "source": [
        "image_generator = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    zoom_range=0.1,\n",
        "    shear_range=0.6,\n",
        "    # rescale=1. / 255.,\n",
        "    width_shift_range=0.15,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=False,\n",
        "   # validation_split=0.2\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kX8I4FnNrYFR"
      },
      "source": [
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oRFFSVpk2KT"
      },
      "source": [
        "# 이미지 generator 실행함수\n",
        "\n",
        "하나의 row마다 augment size만큼 증가 시킨다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N51Ak72Akdt_"
      },
      "source": [
        "\n",
        "def augments(gen, np_img, np_answer):\n",
        "  #augment_size = 30000\n",
        "  arr_size = len(np_img) * augment_size\n",
        "  result = [] * arr_size\n",
        "  result_answer = [] * arr_size\n",
        "  for x in range(len(np_img)):\n",
        "    gen_result = gen.flow(np.tile(np_img[x].reshape(28 * 28 * 1), augment_size).reshape(-1, 28, 28, 1),\n",
        "                                    np.zeros(augment_size), \n",
        "                                    batch_size=augment_size, \n",
        "                                    shuffle=False).next()[0]\n",
        "    answer = np_answer[x]\n",
        "    for j in gen_result:\n",
        "      result.append(j)\n",
        "      result_answer.append(answer)\n",
        "\n",
        "  return np.array(result), np.array(result_answer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwqRkbo7BgJ2"
      },
      "source": [
        "# augments(image_generator, x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIymT8r-6Abe"
      },
      "source": [
        "augment_size = len(np.where(y_train == 6)[0])\n",
        "x_augmented = x_train[np.where(y_train == 6)].copy()\n",
        "y_augmented = y_train[np.where(y_train == 6)].copy()\n",
        "\n",
        "x_augmented = image_generator.flow(x_augmented, \n",
        "                                   np.zeros(augment_size), \n",
        "                                   batch_size=augment_size, \n",
        "                                   shuffle=False).next()[0]\n",
        "\n",
        "x_train = np.concatenate((x_train, x_augmented))\n",
        "y_train = np.concatenate((y_train, y_augmented))\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blaFoeu4k9qJ"
      },
      "source": [
        "augment_size = 90000\n",
        "\n",
        "random_mask = np.random.randint(x_train.shape[0], size= augment_size)\n",
        "x_augmented = x_train[random_mask].copy()\n",
        "y_augmented = y_train[random_mask].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5yA8tFbl9_a"
      },
      "source": [
        "x_augmented = image_generator.flow(x_augmented, \n",
        "                                   np.zeros(augment_size), \n",
        "                                   batch_size=augment_size, \n",
        "                                   shuffle=False).next()[0]\n",
        "\n",
        "x_train = np.concatenate((x_train, x_augmented))\n",
        "y_train = np.concatenate((y_train, y_augmented))\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqI-fjO6kku-"
      },
      "source": [
        "#x_train, y_train = augments(image_generator, x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GV8m4BMCkn--"
      },
      "source": [
        "x_train.shape, y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeqvZHJKBtW8"
      },
      "source": [
        "def my_model(filters=[32, 64], kernel_size=(3, 3), pool_size=(2, 2), act='relu'):\n",
        "  width, height, depth, classes = 28, 28, 1, 10\n",
        "  \n",
        "  model = Sequential()\n",
        "  input_shape = (height, width, depth)\n",
        "  chan_dim = -1\n",
        "\n",
        "  for i, x in enumerate(filters):\n",
        "    if i == 0:\n",
        "      model.add(Conv2D(filters=x, \n",
        "                       kernel_size=kernel_size, \n",
        "                       padding='same',\n",
        "                       input_shape=input_shape))\n",
        "    else:\n",
        "      model.add(Conv2D(filters=x, kernel_size=kernel_size, padding='same'))\n",
        "\n",
        "    model.add(BatchNormalization(axis=chan_dim))\n",
        "    model.add(Activation(act))\n",
        "    model.add(Conv2D(filters=x, kernel_size=kernel_size, padding='same'))\n",
        "    model.add(BatchNormalization(axis=chan_dim))\n",
        "    model.add(Activation(act))\n",
        "    model.add(MaxPooling2D(pool_size=pool_size))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  \n",
        "  model.add(Dense(512))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation(act))\n",
        "  model.add(Dropout(0.4))\n",
        "\n",
        "  model.add(Dense(128))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation(act))\n",
        "  model.add(Dropout(0.4))\n",
        "\n",
        "  model.add(Dense(classes))\n",
        "  model.add(Activation('softmax'))\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X05wNovEJdfQ"
      },
      "source": [
        "x_train.shape, x_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRSZOn1MKjCv"
      },
      "source": [
        "#값 정규화\n",
        "ㅇ이미지 데이터 generator에서 작업하기 때문에 주석처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AT7zx8g_KY6g"
      },
      "source": [
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d73PGbg4T5vu"
      },
      "source": [
        "# 원핫 인코딩으로 변경"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieu7so0yUA43"
      },
      "source": [
        "classes = len(np.unique(y_train))\n",
        "y_train = to_categorical(y_train, num_classes=classes)\n",
        "y_test = to_categorical(y_test, num_classes=classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYGf1-aMNR0q"
      },
      "source": [
        "# 모델 저장 기준이 되는 콜백 import\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSStpywKOPlT"
      },
      "source": [
        "# 체크할 포인트\n",
        "check_point = ModelCheckpoint('best_model.h5', \n",
        "                              monitor='val_accuracy',\n",
        "                              verbose=1,\n",
        "                              save_best_only=True)\n",
        "\n",
        "# 이른 스탑을 위한 클래스\n",
        "early_stopping = EarlyStopping(monitor='val_accruacy',\n",
        "                               min_delta=0,\n",
        "                               patience=7,\n",
        "                               verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIJ0BnbiNzMA"
      },
      "source": [
        "# 모델 컴파일 및 학습\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiUrlUU5QgZK"
      },
      "source": [
        "bat_size = 128\n",
        "epochs = 33\n",
        "lr = 0.001\n",
        "bs = 128\n",
        "ep = 50\n",
        "\n",
        "\n",
        "optimizer = Adam(learning_rate=lr, \n",
        "                #decay=lr / ep, \n",
        "                beta_1=0.9,\n",
        "                beta_2=0.999)\n",
        "\n",
        "\n",
        "model = my_model()\n",
        "\n",
        "model.compile(optimizer=optimizer,\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train,\n",
        "                    y_train,\n",
        "                    batch_size=bs,\n",
        "                    validation_split=0.3,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    epochs=ep,\n",
        "                    verbose=2,\n",
        "                    callbacks=[check_point, early_stopping])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FZStfNbjt89"
      },
      "source": [
        "loaded_model = load_model('best_model.h5')\n",
        "\n",
        "prob_pred = loaded_model.predict(x_test)\n",
        "pred = loaded_model.evaluate(x_test, y_test)\n",
        "print(pred)\n",
        "\n",
        "prob_label = prob_pred.argmax(axis=-1)\n",
        "\n",
        "np.savetxt('y_pred.csv', prob_label, fmt='%d')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMjBHx0I4Bqs"
      },
      "source": [
        "# show a nicely formatted classification report\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "label_names = [\"top\", \"trouser\", \"pullover\", \"dress\", \"coat\", \"sandal\", \"shirt\", \"sneaker\", \"bag\", \"ankle boot\"]\n",
        "\n",
        "print(\"[INFO] evaluating network...\")\n",
        "print(classification_report(y_test.argmax(axis=1), prob_pred.argmax(axis=1), target_names=label_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQn9-LPl8tMz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}